---
title: 强化学习(简介)
date: 2018-12-23 21:30:02
tags: 
- 机器学习
- 强化学习
categories: 
-  机器学习
-  强化学习
---

#### Reforcement Learning
- 什么叫RL
  和监督学习的区别是，Agent会通过观察(Observation)当前环境(Environment),获得当前的State，接着Action后能够得到Environment的反馈：Reward，然后Agent根据Reward再调整做出正确的Action去改变Environment
  - Agent:学习主体，可以是一个NN或者其它，作用：Learns to take actions maximizing expected reward
  - Environment:外部环境，比如下围棋就是对手，玩电玩的时候，就是主机
  - State：Agent通过观察获取到的当前环境的输入状态
  - Action：根据当前状态做出的动作，当然此动作会影响Environment，比如下围棋的时候会影响对手下一步落棋
  - Reward:做出action，当前Environment给予的评价
  {% asset_img resources/089167ED63C58D955C18087D366BF2A3.jpg %}
  {% asset_img resources/260B08ED8010A47991A9DF4BE29D1E57.jpg %}

- Look for a function
  {% asset_img resources/B620070EEBB401F2C731FCC8E23FAB60.jpg %}

- 举例
  - state:输入为当前画面(像素值)
  - action:左移，右移或者开火
  - reward:Score
  {% asset_img resources/F64DEE320E6175A281141261369F122A.jpg %}


- outline
  {% asset_img resources/813056D16FC4398AB40E7F56841C8828.jpg %}
  

#### Policy-based Approach(Learining an Actor)
- 总体分三步
  {% asset_img resources/478F4C2660ABFC1B5E675896C8571F84.jpg %}
  - First step：
    {% asset_img resources/E5859C0699A3CB2FB7D06715DEF7C293.jpg %}

  - Second step：
    {% asset_img resources/035061213843511C0A7561A1F0E06792.jpg %}

  - Third step(pick the best function)：
    {% asset_img resources/FEB8FFD784F357B5CFE54CCD02E5A05B.jpg %}

- Policy Gradient作用
如果Actor,Env,Reward都看做是DNN，那么本质上就是一个NN网络，求极值就很容易。但实际上Env，Reward不是一个NN网络，所以就不能进行微分，不能求出极值，解决办法就是用policy gradient进行处理
{% asset_img resources/61A9812B02E141127554B513DA22F2CA.jpg %}


#### Value-based Approach(Learning a critic)
- 是评价一个agent(actor)的好坏，$V^{\pi}(s)$表示给定一个Agent(Actor:$\pi$)，并给定一个State:s，到最后游戏完成后，得到最后的reward expects的值
  - 比如下面坐标的图，有很多怪，到游戏结束期望分数就容易获得比较高的值，右边图就比较小(因为这个图看到到最后游戏结束，能杀的怪已经很少了)
  - 同一个state，不同的actor，那么$V^{\pi}(s)$值也会不一样
{% asset_img resources/84B70D51C472CE9220B50917CF343EBE.jpg %}


- How to estimate $V^{\pi}(s)$
  - Monte-Carlo,观察到两个state,$S_a,S_b$，并且最后episode结束，Greed为$G_a,G_b$，这个时候，只需要让$V^{\pi}(s_a)\approx G_a,V^{\pi}(s_b)\approx G_b$
  {% asset_img resources/314B5DFA5F6A0AEEACBE25BEEC618D05.jpg %}

  - Temporal-difference,不会等到episode才开始计算(不用等到游戏结束就可以更新参数)，原理是$V^{\pi}(s_t),V^{\pi}(s_{t+1})$中间是相差的$r_t$,所以只需要$V^{\pi}(s_t)-V^{\pi}(s_{t+1})\approx r_t$
  {% asset_img resources/D113811A3EE6DF3759AC23138C68A4C6.jpg %}

- Q Learning
  - Another Critic
    - 输入为state和action,可以对所有action做穷举（如果不能穷举，其实还有其他的方法的），看哪一个reward得分最高(得到Q function)
  {% asset_img resources/26D289911C892B3832B7E88BDA9E3848.jpg %}

  - 每一次都找最大的Qfunction数值的action a
  {% asset_img resources/1060DC495D95BBDB0CD2FB19749CD14E.jpg %}

#### Actor+Critic
- 核心原理：不会像Actor那样跟着环境去学习，因为环境变化是比较多的，所以Actor-Critic是跟着critic去学习
  {% asset_img resources/EA44E3EFC2F7B6582C5207FB6461F7EE.jpg %}
  {% asset_img resources/7BB0A900A5C3D4D5567050C7A7D0FA0A.jpg %}
- A3C
  {% asset_img resources/1FDE16A21B149590A05D2335FFC45D90.jpg %}

#### Inverse Reforcement learning
- 背景：其实生活中的大多数场景，都不好找reward的，不像围棋或者是电玩，有很明确的规则.
  - 比如如果交通违规，如何处罚等。还比如说让机器人放盘子，之前并没有告诉摔坏盘子会扣分，机器人就不知道
- 原来的方案：
  {% asset_img resources/ADBCCF56067A9D873705B18C483D8B4D.jpg %}
- Inverse Reforcement learning方案：
  - 正好相反，并不知道Reward Function，是通过学习学习到Reward Function后，然后使用它选择出最好的actor
  {% asset_img resources/57B1F1A69DD14E80166F5776FB4E9CF5.jpg %}
- 步骤：
  - expert和IRL都会自己学习，获得最终Reward，我们假设专家的Reward总数比学习到的好
  - 然后从中我们找到一个最好的Reward Function R
  - 再通过R去得到Actor $\pi$
  - 要注意下是，如果规则变化了，那么下面的循环圈需要不断的重新循环取学习
  {% asset_img resources/DE2E7716C70854CC5CE59AC271378ED4.jpg %}

- 发现和GAN比较像
{% asset_img resources/FE61F494902468450399EBA1D4B37642.jpg %}