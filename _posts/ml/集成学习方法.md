---
title: 集成学习方法
date: 2018-07-24 23:09:10
tags: 
- 机器学习
- 机器学习基础
categories: 
- 机器学习
- 机器学习基础
---

#### 个体与集成
集成学习（ensemble learning）的一般结构：先产生一组“个体学习器”（individual learner），再用某种策略将他们结合起来
{% asset_img resources/4B0EDAB496C74765B1C89797A91E5BB2.jpg %}
- 集成也可包含不同类型的个体学习器
  - 在一般的经验中，如果把好坏不等的东西掺到一起，那么通常结果会是比坏的好一些，比好的要坏一些。集成学习把多个学习器结合起来，如何能获得比最好的单一学习器更好的性能呢

- 考虑一个简单的例子：在二分类任务中，假定三个分类器在三个测试样本的表现如下图所示
  - 集成学习的结果通过投票法（voting）产生，即“少数服从多数”。
  - 这个简单的例子显示出：要获得好的集成，个体学习器应“好而不同”。
    - 个体学习器要有一定的<font color='blue'>"准确性"</font>，即学习器不能太坏
    - 而且要有<font color='blue'>"多样性"（diversity）</font>，即学习器之间有差异。
    - 事实上，如何产生并结合“好而不同”的个体学习器，恰是集成学习研究的核心
  {% asset_img resources/EC147BED87BB6C5FE7B43608C8F250F2.jpg %}

- 多学习器结合的好处
  - 从统计的方面看，由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能，此时若使用单学习器可能因误选而导致泛化性能不佳，结合多个学习器减小这一风险；
  - 从计算的方面来看，学习算法往往会陷入局部极小，有的局部极小点所对应的泛化性能可能很糟，而通过多次运行之后进行结合，可降低陷入糟糕局部极小点的风险；
  - 从表示的方面来看，某些但学习器则肯定无效，而通过结合多个学习器，由于响应的假设空间有所扩大，有可能学得更好的近似
  {% asset_img resources/C648EEAAFD8FB8DF3F291B6B0A998214.jpg %}

- 结合不同子模型的分类
{% asset_img resources/BC8AEFE15619E77CB8F6BF1C2478097F.jpg %}

#### Bagging
各个子模型训练的时候是随机在样本库中抽取部分样本，并行训练，最后的多个模型参数取平均值
- 随机抽取样本(有放回的)
- 并行训练模型，各个模型之间无影响
- 最后样本合成时，以平均值参数进行合成$F(x)=\frac 1m\sum_{i=1}^mf_i(x)$
  {% asset_img resources/2C16474FB5F7CE8CDAFFAFA3A1E57777.jpg %}

#### Boosting
Boosting是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制类似：
- 先从初始训练集训练出一个基学习器，再根据基学习器的表现对<font color='blue'>训练样本分布进行调整</font>，使得先前基学习器做错的训练样本在后续收到更多的关注<font color='blue'>(其实就是改变权重，对判断正确的样本进行降权，判断错误的进行加权)</font>
- 然后基于调整后的样本分布来训练下一个基学习器；
- 如此重复进行，直到基学习器数目<font color='blue'>达到事先指定的值T</font>，最终将这T个学习器进行加权结合
特点：
- 每个学习区直接相互影响，串行化学习，最终的模型是对各个学习器加权求和,$F(x)=\sum_{i=1}^m\theta _if_i(x)$
{% asset_img resources/E593F3A40EE6A8026D8ECB26C37FA5D2.jpg %}