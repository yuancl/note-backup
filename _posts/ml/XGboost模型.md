---
title: XGboost模型
date: 2019-05-17 07:19:11
tags: 
- 机器学习
- 集成学习
categories: 
- 机器学习
- 集成学习
---

https://blog.csdn.net/guoxinian/article/details/79243307
https://blog.csdn.net/matrix_zzl/article/details/78635221#2-xgboost%E5%8E%9F%E7%90%86%E6%8E%A8%E5%AF%BC

#### boosting集成框架
- boosting集成是后一个模型是对前一个模型产生<font color='blue'>误差信息进行矫正</font>
- gradient boost更具体，新模型的引入是为了<font color='blue'>减少上个模型的残差(residual)</font>，我们可以在<font color='red'>残差减少的梯度(Gradient)方向上</font>建立一个新的模型
- 框架算法：
  {% asset_img resources/0317F68CB4256A79486DF6B5D164BE3F.jpg %}
  - 1.设定函数初始值F0，为一个恒值函数，论文中基于变量优化出恒值，实际上也可以给定任意值或者直接为0
  - 2.泛函优化
    根据参数MM，进行MM次迭代，不断将当前函数$F_{m−1}$往最优函数F∗空间上逼近，<font color='blue'>逼近方向就是当前函数下的函数负梯度方向</font>-$\delta L(y,F)$,由于优化函数，而非变量，本质上属于泛函优化
  - 3.每次迭代计算出函数负梯度，<font color='red'>基于训练数据构建模型来拟合负梯度</font>。原则上可以选择任何模型：树模型，线性模型或者神经网络等等，很少框架支持神经网络，推测：神经网络容易过拟合，后续函数负梯度恒为0就无法继续迭代优化下去。如果用树模型进行拟合，就是我们熟悉的CART建树过程
- 泛函优化与变量优化
  {% asset_img resources/74A523412DEFC18E07ABF4E2D8EABAA5.jpg %}
- 和bagging比较并行性
  谈到集成学习，不得不说bagging集成，比如随机森林
  - 1）建树前对样本随机抽样（行采样）
  - 2）每个特征分裂随机采样生成特征候选集（列采样）
  - 3）根据增益公式选取最优分裂特征和对应特征分裂值建树。
    - 建树过程完全独立，不像boosting训练中下一颗树需要依赖前一颗树训练构建完成，因此能够完全并行化。Python机器学习包sklearn中随机森林RF能完全并行训练
    - 而GBDT算法不行，训练过程还是单线程，无法利用多核导致速度慢。希望后续优化实现并行，Boosting并行不是同时构造N颗树，<font color='blue'>而是单颗树构建中遍历最优特征时的并行</font>，类似XGBoost实现过程。随机森林中行采样与列采样有效抑制模型过拟合，XGBoost也支持这2种特性，此外其还支持Dropout抗过拟合。

#### XGboost改进-Loss方程
- Loss进行泰勒二阶展开并且加入了惩罚项
  {% asset_img resources/DBE01DCBB543EA600C500B25827EA668.jpg %}
  - 目标函数通过二阶泰勒展开式做近似。传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。注：支持自定义代价函数，只要函数可一阶和二阶求导
    - [泰勒展开理解](https://blog.csdn.net/s12117719679/article/details/87883168) 
      - 如何让两个人运动轨迹一样？
        就是让两个人的速度，加速度，加速度的加速度...都一致。那么翻译成数学语言，也就是两条曲线想要一样，那么在某一点的一阶导数，二阶导数，三阶导数，四阶导数....n阶导数也相同，就说这两条曲线是相同的。也就是泰勒展开式的核心思想

  - 定义了树的复杂度，即xgboost在代价函数里加入了正则项，用于控制模型的复杂度，<font color='blue'>正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和</font>。代替了剪枝。
  {% asset_img resources/C0930E7CC44A216A76354A27B6EAE3F8.jpg %}


#### XGboost改进-特征树分割点选择(精确法与近似方法比较)
- XGBoost在单机默认是exact greedy，搜索所有的可能分割点。分布式是dynamic histogram
- 精确算法由于需要遍历特征的所有取值，计算效率低，适合单机小数据，对于大数据、分布式场景并不适合
- 可并行的近似直方图算法，用于高效地生成候选的分割点。用于加速和减小内存消耗
  - 分裂结点处通过结构打分和分割损失动态生长。结构分数代替了回归树的误差平方和。


#### XGboost改进-并行化
支持并行化处理。xgboost的并行是在特征粒度上的，<font color='blue'>在训练之前，预先对特征进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量</font>。在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行，即在不同的特征属性上采用多线程并行方式寻找最佳分割点。

#### XGboost改进-其他
- 自定义损失函数
- 可以处理稀疏、缺失数据(节点分裂算法能自动利用特征的稀疏性),可以学习出它的分裂方向，加快稀疏计算速度。
- 列抽样（column subsampling）[传统GBDT没有]xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性
- Shrinkage(缩减)，相当于学习速率(xgboost中的eta)[传统GBDT也有]
- 传统GBDT以CART作为基分类器，xgboost还支持线性分类器