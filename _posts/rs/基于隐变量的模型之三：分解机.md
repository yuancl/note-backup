---
title: 基于隐变量的模型之三：分解机
date: 2019-02-27 07:19:29
tags: 
- 推荐系统
- 推荐模型简介
categories: 
-  推荐系统
-  推荐模型简介
---

#### 矩阵分解和基于回归的隐变量模型存在哪些问题
- 发展脉络 
  - 首先，矩阵分解主要解决了两个问题，那就是从一个大矩阵降维到两个小矩阵，并且寄希望这两个小矩阵能够抓住用户和物品的相关度。
    - 然而，单纯的矩阵分解无法融入很多用户和物品的特性
  - 这就引导我们开发出了基于回归的矩阵分解。
    - 所谓的回归部分，也就是从显式特性出发，建立从显式特性到隐变量之间关系的流程，从而使我们能够把更多的信号放进模型中。
    - 在一定程度上，基于回归的隐变量模型实现了把显式变量和隐变量结合的目的
      - 但是这类模型的学习过程非常麻烦。实际上，因为这类模型复杂的训练流程，其在实际应用中并不常见。
  
那么，有没有其他思路来统一显式变量和隐变量的处理方式呢？


#### 分解机
- 核心原理
  - 核心就是认为<font color='red'>需要预测的变量（这里我们依然讨论评分）是所有显式变量的一个回归结果</font>
    - 分解机直接借鉴了这一点，也就是说，<font color='blue'>分解机的输入是所有的显式变量</font>
    
- 个人理解
  - 这种推荐模型本质可以看做是个回归模型：需要预测的变量（这里我们依然讨论评分）是所有显式变量的一个回归结果 
  - <font color='purple'>分解机本质就是增加了更多的特征，比如各个显示变量的两两乘积生成新特征
  - 适当增加模型复杂度，泛化性强：结合了隐变量，比如显示变量的两两乘积用隐变量的乘积来表示(一种显示变量就用隐变量vector表示)</font>
  
- 基本思想
  - 第一步:把用户的年龄和物品的种类直接当作特性输入到模型中
  - 第二步:分解机是把这两个特性的数值进行乘积，当作一个新的特性，然后进一步处理这种两两配对的关系
    - 增加了特征(<font color='blue'>显示变量两两配对</font>)
      分解机在对待显式变量的手法上更进了一步，那就是不仅直接对显式变量进行建模，还对显示变量的两两关系进行建模。当然，在原始的论文中，分解机其实还可以对更加高维的关系进行建模，我们这里局限在两两关系上
      - <font color='blue'>把原始特性进行两两配对是构建模型的一种重要的方法，特别是对于非深度学习模型，需要自己做特征工程的模型</font>
      - 两两配对问题
        - 一个问题就是<font color='blue'><strong>特性空间会急速增长</strong></font>
        - 另一个更严重的问题就是，如果我们的单独特性中，有一些是“类别特性”（Categorical Feature），那么在两两配对之后就会产生大量的 0，从而变成一个巨大的<font color='blue'>稀疏矩阵</font>

#### 如何建模(如何解决上面问题)
- 分解机利用了<font color='blue'>矩阵分解的降维思路</font>
- 我们不对一个稀疏矩阵直接建模，而是把这个稀疏矩阵分解之后再进行建模
- 就是先假定，<font color='blue'>所有特性都对应一个隐变量向量，两个显式特性的乘积是两个特性的隐变量的点积</font>
  - 也就是说，我们把两个显式特性的乘积分解为了两个向量的乘积。这样，我们就不需要直接表示原来的稀疏矩阵。</p>
- 在这样的思路下，分解机成功地把隐变量和显式变量结合到了一起
  - 当我们的显式特性仅仅是用户 ID 和物品 ID 的时候，分解机的表达退回了最原始的矩阵分解
  - 也就是说，矩阵分解其实可以表达成为特性的两两作用矩阵的分解
- 在原始的论文中，作者还用分解机模拟了好几种流行的模型
- 虽然也是为了建立从显式特性到隐变量的关系，但是对比基于回归的矩阵分解而言，分解机的训练过程大大简化了。在实际应用中，我们经常使用“<strong>随机梯度下降</strong>”（SGD, Stochastic Gradient Descent）来对分解机直接进行求解

#### 思考问题
- 分解机能够解决“冷启动”的问题吗
  - 个人认为是可以解决的：因为本质就是一个回归模型，由隐变量得到显变量的映射。然后映射到指标(比如说点击率，评论得分)
  - 所以在冷启动问题上，对缺失的变量可以看其分布，然后拿到期望值，这样来处理