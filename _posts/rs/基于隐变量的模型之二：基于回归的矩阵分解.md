---
title: 基于隐变量的模型之二：基于回归的矩阵分解
date: 2019-02-26 08:15:19
tags: 
- 推荐系统
- 推荐模型简介
categories: 
-  推荐系统
-  推荐模型简介
---

#### 基础矩阵分解问题
- 第一，矩阵分解的矩阵仅仅是对用户和物品的喜好进行了“编码”（Encode），但在融合多种不同的推荐元素方面，表现却很一般
- 第二，矩阵分解的核心是学习用户的隐向量和物品的隐向量。原则上，这两类隐向量的学习仅能通过训练过程获得。
  - 我们无法获得新来用户或者新来物品的隐向量了，因为这些用户和物品并不在训练集里
  - 冷启动问题
    在推荐系统中，这种情况就叫作不能处理“冷启动”（Cold Start）问题，也就是不能处理“冷”用户和“冷”物品。在这样的场景下，直接使用矩阵分解就会有问题


#### 基于回归的矩阵分解
- 首先，有一组用户特性和物品特性来表述每一个用户和物品。这些特性不是隐变量，是显式表达的特性
  - 用户特性比如用户的年龄、性别、经常出现的地区、已经表达了喜好的类别等
  - 物品特性比如物品的种类、描述等等
  - 这两组显式的特性就是为了解决我们刚才说的第一个问题(融入更多元素)，矩阵分解无法抓住更多的信号。
- 现在我们有两个独立的部分
  - 一个是基于矩阵分解的部分，这一部分是分解一个已知的评分矩阵，从而学习到用户和物品的隐向量
  - 另外一个部分，就是用户特性和物品特性
- 关联两部分
  用户的隐向量，其实是从用户的显式特性变换而来的
  - <font color='blue'>我们建立一个从显式特性到隐向量的回归模型，使得隐向量受到两方面的制约：从评分矩阵的分解得来的信息和从显式特性回归得来的信息</font>
- 不怕冷启动
  不再怕“冷启动”了</strong>。或者说，在有一部分“冷启动”的情况下，这样的模型可以处理得更好。原因就是我们使用了显示特性来回归隐向量
- 贝叶斯角度理解
  我们还可以从贝叶斯的角度来理解基于回归的矩阵分解。把用户的隐向量和物品的隐向量看作是两个随机变量。我们可以认为这些随机变量加上了先验概率分布。只不过，这个先验概率分布的均值不是我们经常使用的 0，而是一个以回归函数的结果为均值的高斯分布，这个回归函数就是我们由显式特性得到的。本质上，我们认为显示特性的某种变换成为了隐向量的先验信息


#### 小结
- 第一，我们简要介绍了矩阵分解的一些问题
- 第二，我们详细介绍了基于回归的矩阵分解的基本思路，以及这样的模型如何解决了传统矩阵分解关于“冷启动”的难题
- 第三，如何学习的问题，需要查阅其他资料