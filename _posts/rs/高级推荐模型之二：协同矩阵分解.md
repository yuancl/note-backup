---
title: 高级推荐模型之二：协同矩阵分解
date: 2019-02-28 07:20:21
tags: 
- 推荐系统
- 推荐模型简介
categories: 
-  推荐系统
-  推荐模型简介
---


#### 为什么需要协同矩阵分解
矩阵分解的核心就是通过矩阵，这个二维的数据结构，来对用户和物品的交互信息进行建模（如何融入更多信息）
- 因为其二维的属性，矩阵往往只能对用户的某一种交互信息直接进行建模，这就带来很大的局限性
- 思路一，就是通过建立显式变量和隐变量之间的回归关系，从而让矩阵分解的核心结构可以获得更多信息的帮助。
- 思路二，则是采用分解机这样的集大成模型，从而把所有的特性，都融入到一个统一的模型中去。
- 思路三，就是我们这周已经讲到的，利用张量，把二维的信息扩展到 N 维进行建模

#### 如何组织更多的二元关系
除了用户和物品这样很明显的二元关系以外，还有其他也很明显的二元关系，如何把这些二元关系有效地组织起来，就变成了一个有挑战的任务
- 在前面的思路里面可以看到，我们似乎需要选择一个主要的关系来作为这个模型的基础框架，<font color='blue'>然后把其他的信息作为补充</font>。在这样两类关系中，选择哪一个作为主要关系，哪一个作为补充关系，就显得有一点困难了
- 这也就让研究人员想出了协同矩阵分解的思路

#### 协同矩阵分解的基本思路
- 协同矩阵分解的基本思路其实非常直观，那就是有多少种二元关系，就用多少个矩阵分解去建模这些关系
- 如果协同(如果让这多个矩阵产生关系?)
  - 理论上基于矩阵分解得到的隐变量，相互是独立的，没有关系的
  - 我们必须有其他的假设。这里的其他假设就是，两组不同的用户隐变量其实是一样的。也就是说，我们假设，或者认定，用户隐变量在用户与用户的关系中，以及在用户与物品的关系中，<font color='blue'>是同一组用户隐变量在起作用</font>
    - 说得直白一些，我们认定从两个矩阵分解出来的两组来自同一个因素（这里是用户）的<font color='blue'>隐变量是完全一样的</font>。用更加学术的语言来说，这就是将两组矩阵分别投影到了相同的用户空间和物品空间
    
- 优点
  我们使用“相同隐变量”这样的假设，可以把这些关系都串联起来，然后减少了总的变量数目，同时也让各种关系互相影响
- 缺点
  - 使用同样的一组隐变量去表达所有的同类关系，这样的假设存在一定的局限性，比较难找到
  - 不同关系的数据量会有很大的差距。比如，用户和物品关系的数据总量可能要比用户与用户的多。所以，由于用户和物品关系的数据多，两个矩阵分解用的同一组用户隐变量，很可能会更多地解释用户和物品的部分，从而造成了学到的隐变量未必能够真正表达所有的关系
  

#### 问题
从概念上来看，协同矩阵分解和张量分解之间有怎样的关系？是不是所有的张量分解都可以化为多个协同矩阵分解呢
- 我的理解是ok的