---
title: 第二门课-第二周
date: 2018-09-10 22:40:03
tags:
categories: 
-  深度学习
-  吴恩达课程总结
---

### 第二周

第二周主要学习如何优化算法，优化算法能够让你快速训练模型，主要分三类：

* Mini-batch
* 指数加权平均(Momentum，RMSprop，Adam)
* 学习率衰减

#### 2.1 Mini-batch梯度下降&理解

* 迭代中的cost一定是逐渐减小
* 三种梯度下降方法(Batch，随机梯度，Mini-Batch)，cost图如下： 
  * 蓝色为常用Batch方式，梯度不会抖动，缺点是每次迭代是全部样本比较慢，一次迭代只改变一次梯度
  * 紫色为随机梯度，每次一个样本，这种方式cost的计算抖动比较大，在最优点附近也是抖动，并不会停留在此
  * 绿色为Mini-batch方式： 
    * 也会出现抖动，因为Cost的计算是随意加入的样本，而每次epom样本都在变化。
    * 它也不一定会在很小范围内收敛或者波动，这种情况就可以考虑减小学习率了
    * Mini-batch size选择，一般选择2n，64--512比较常见
  * 三种方式如何选择，主要还是看样本量，一般随机梯度不会选，如果样本量\<2000，选择Batch，否则选择Mini-batch
 {% asset_img resources/3BC2DF36EFE921C9C0EEB02EBBEB579D %}

#### 2.3 指数加权平均数 & 理解

- 也叫做移动平均值，就是计算局部范围的平均值：$v_t=\beta v_{t-1}+(1-\beta)\theta_{t-1}$(公式1),其中$\frac 1{(1-\beta)}$代表的所要计算的平均范围(这结论数学推导可查阅资料)，比如计算温度的指数加权平均>值 $\theta=0.9$,那么$\frac 1{(1-\beta)}=10$,表示计算的10的平均温度

#### 2.5 指数加权平均的偏差修正

* **蓝色**是没有添加指数加权平均的各个离散点，**红色**为$\beta=0.9$的加权平均值曲线，**绿色**为$\beta=0.98$加权平均值曲线，**紫色**为$\beta=0.98$并且进行修正后的加权平均值曲线
{% asset_img resources/BE291D0E3476361A7A64454978286DA9 %}

* 上图可以看见绿色和紫色的区别是在初始阶段，通过公式(1)计算得到的$v_0,v_1$和实际值差别是比较大的，造成了前面几个数不太准确,方法：使用$\frac{v_t}{1-\beta_t}$而不是直接用$v_t$进行评估,可以看见初期比较准确，然后后期$\beta_t$很小,所以$\frac{v_t}{1-\beta_t}\approx{v_t}$

#### 2.6 动态梯度下降

- **目的**：想让在x方向上梯度变化大一点，但是在纵轴方向，来回抖动，想要修正y方向上的抖动。**方案**：在x，y轴分别使用2.5中的指数加权平均法，对y轴的**正负抖动进行抵扣平均**，达到y轴抖动较小目的:
$v_{dw}=\beta v_{dw-1}+(1-\beta)d_{dw-1}$
W=W-$\alpha v_{dw}$
$v_{db}=\beta v_{db-1}+(1-\beta)d_{db-1}$
b=b-$\alpha v_{db}$
 {% asset_img resources/B75553739F2911D13CCF981D2AC5D707 %}
- 假设W，b分别表示x，y轴。看下面计算公式，其中学习率$\alpha$也是有影响的，会在下面介绍
 {% asset_img resources/DBAC055D9744073F23A30ECC596CA307 %}

#### 2.7 RMSprop

- RMSprop是微分平方加权平均:
$S_{dw}=\beta S_{dw-1}+(1-\beta)d_{dw-1}^2$  **1式**
W=W-$\alpha \frac{dw}{\sqrt S_{dw} + \epsilon}$ **2式**($\epsilon$避免非0,一般$10^{-8}$)
$v_{db}=\beta v_{db-1}+(1-\beta)d_{db-1}^2$
和动态梯度下降区别：
    - 并不是正负抵消，这些计算的是微分平方加权平均，然后计算梯度变化的时候除以平方根
    - 原理**(我理解就是将大值调小，小值调大)**：
        - 如果dw大，那么1式结果$S_{dw}$也会比较大，但2式结果W就会小
        - 如果dw小，那么1式结果$S_{dw}$也会比较小，但2式结果W就会大
- RMSprop我理解就是将大值调小，小值调大,个人觉得Momentum(平均后正负抵消)更有效
 {% asset_img resources/F8B3F5FAEE78DF176AD2D9D1EA759F73 %}

#### 2.8 Adam优化算法

* Momentum与RMSprop的结合
* 其中有多个超参数及默认值α,β1​(0.9),β2​(0.98),ϵ(10−8)
 {% asset_img resources/01107664832D4483D8213338204FC312 %}

#### 2.9 学习率衰减

* 如果学习率α一直是一个值，在使用Mini-batch方法的时候，就会在收敛值附件波动，根本原因就是在收敛期α过大，变化过多的原因
* 动态变化α值，开始α希望较大，收敛时α逐渐变小，类似于绿色曲线，在收敛值较小范围波动

{% asset_img resources/9E1821378657CB076651F5DC38160B3C %}

#### 2.10 局部最优问题

* 担忧深度学习优化算法总是困在极差的局部最优
* 通常梯度为0的点并不是局部最优点，实际上成本函数的零梯度点，即鞍点
 {% asset_img resources/68F0C0CA5B07EF344E68284131BA8147 %}
* 普通优化算法一般都是沿着蓝色点坡度向下，比较难走出平稳期。为什么Momentum，RMSprop和Adam能尽早往下坡出平稳期？
 {% asset_img resources/FA0191259C4FB3E25C573FEA9CF7DEDF %}

