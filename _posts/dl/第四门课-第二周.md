---
title: 第四门课-第二周
date: 2018-11-14 21:10:09
tags: 
- 深度学习
- 深度学习吴恩达课程
categories: 
-  深度学习
-  吴恩达课程总结
---

[习题代码](https://github.com/yuancl/dl-algorithm/tree/master/4-ConvolutionNeuralNetworks/Week2/ResNets)
##### LeNet5

* 它是针对灰色图片训练的
* 架构：CONV-POOL-CONV-POOL-FC-FC-SOFTMAX
* CONV使用5x5 filter，POOL使用2x2，padding都为0，图片宽高在逐渐减小，信道数在逐渐增大
* 还有一种模型多卷积，一个池化，多卷积，一个池化...现在也经常使用
* 经典LeNet5使用了非常复杂的计算方式，每个过滤器都采用和输入模块一样的通道数量，现在一般不会这么做了
* 使用的是sigmod，不是ReLU
 {% asset_img resources/FFB32954180F01A43DAE62D2E79B442D %}

##### AlexNet

* 之前在语音方面已经得到了很大的进展，这是这篇论文，让人们觉得深度学习能够对CV有很大帮助
* AlexNet是更大的网络，大约6000万个参数，而LeNet5大约6万个参数
* **AlexNet能够处理非常相似的基本构造模块(没有理解)？**
* 比LeNet表现另一个原因是使用了ReLu激活函数
* Multiple GPUs:Alex用了非常复杂的方法使这些层拆分到两个不同的GPU上
* LRN(Local Response Normalization)：选取一个位置，贯穿整个通道，得到256个数字，并进行归一化。局部归一化的动机是，对于13x13的图像中的每个位置来说，我们可能并不需要太多的高激活神经元。也没有理解
* 网络特点：卷积核除了第一个，全部采用same卷积
 {% asset_img resources/95ED2FBAA8AEC165904C8B9DE4A6670A %}

##### VGG-16

* 和LeNet比，它**非常规整**
* 简化了神经网络的结构，卷积层采用same卷积，信道数翻倍。池化层宽带高度都减半(每次池化后宽高缩小一倍，每次卷积后通道数也增加一倍)
* 16代表包含有16个卷积层和全连接层，确实是很大的网络。但是非常规整，都是几个卷积后跟着一个压缩图像大小的池化层，这一点很吸引人。信道数64-128-256-512...可能认为已经足够大了，就没有再翻倍了
* 缺点是需要训练的特征数量非常巨大
* 还有VGG-19，是更庞大的网络，但效果其实和VGG-16差不多
 {% asset_img resources/2C14B193C833B6DFE0463405C9D003D5 %}

##### ResNets
[可参看习题](https://github.com/yuancl/dl-algorithm/tree/master/4-ConvolutionNeuralNetworks/Week2/ResNets)


- **解决问题**(注意和梯度优化算法指数加权平均的区别)
  网络越深，越容易出现梯度消失问题，因为在反向传播算法就是梯度的时候，和各个层级的系数相关，相乘，所以很容易出现接近于0去情况，所以一开始就很快出现梯度消失问题，导致学习(learning speed)非常慢，性能很差(图上可以看见，越是前面的层级问题越严重)
  {% asset_img resources/61F21AE03F6475441BFAE38139170DFE.jpg =561x267 %}

* **网络结构：**

  * a[l+2]=g(z[l+2]+a[l]),其中a[l]就是残差块,是插在非线性变换前
  * 对于一个普通网络来说深度越深优化算法越难训练，训练误差越来越多。但ResNet表现比较好
  * 因为这种方式有助于解决梯度消失和梯度爆炸的问题，让我们训练更深的网络的同时，又能保证性能
 {% asset_img resources/2BDA2C468464C84C78603227B52C78E3 %}
* **为什么能work？**

  * a[l+2]=g(z[l+2]+a[l])=g(w[l+2]∗a[l+1]+b[l+2]+a[l])可见，当使用L2正则化或者其他方法，导致权重w会不断衰减(**个人理解就容易出现梯度消失问题**)，我们直接让w[l+2]=0并且b[l+2]=0，有了残差块，所以a[l+2]=g(a[l])=a[l](因为Rele(x)中x\>0)，这样来解决更容易出现的提交消失问题
  * 残差网络学习这种恒等函数非常容易
  * z[l+2]+a[l]由于这里有向量的加减法，所以这两个层的维度必须想办法相对，比较简单的方式是使用same卷积
  * 网络结构：经常多个卷积接一个池化，CONV-CONV-CONV-POOL-CONV-CONV-CONV-POOL......FC-FC-SOFTMAX，无论网络如何，都会进行矩阵维度调整
  * ResNet还提到了使用1\*1的卷积，会在后面讲解
 {% asset_img resources/7F98489A2561372FB25FC4939CA2F778 %}

##### Inception(GoogleNet)

* 网络中的网络已经1x1卷积

  * 改变通道数
 {% asset_img resources/59405B33A45ED9994661CC21A765DDC6 %}
* Incption模块

  * 作用：避免人工确定卷积层中的过滤器类型，或者确定是否需要创建卷积层或池化层
  * 各种大小filter，pool相组合。缺点是计算复杂
 {% asset_img resources/A1892AE97B02F31DB72FC0BC2C6B5AA5 %}
  * 1x1卷积(瓶颈层)，降低通道数，降低计算复杂性(使用瓶颈层来缩小网络)
 {% asset_img resources/943071A79A4B72BD6C85D71BFD5B213B %}
 使用瓶颈层来缩小网络
 {% asset_img resources/2895B32491E5564FDB45A8903147F4F2 %}
* Inception网络

  * Inception module
 {% asset_img resources/CF1D396466A7E5FD698DDE8160C77CF0 %}
  * Inception网络就是将这些模块都组合到一起
 {% asset_img resources/B4825BB37EAF8E3586791DEDE6D02E25 %}
  * 隐藏层和中间层也能进行预测，确保了他们也参加特征计算，并且能够防止网络过拟合 
    * 看途中的分支(编号2，就是通过隐藏层，编号3做的sotmax预测)
  * 当然也有一些比较新的Inception论文，比如Inception V2,V3等，但都是基于这个搭积木的思想
* 如果构建自己的CV网络

  * 使用开源的实现方案：使用别人已经训练好的网络结构及权重，通常能够进展得相当快，用这个作为预训练，然后转换到你感兴趣的任务上
  * 如何迁移？主要根据你自己的数据量大小 
    * Train数据很少，你只需要训练替换后的你自己的softmax层相关参数，而冻结其他层的所有参数 
      * 其他有个技巧，就是把softmax的输入直接存入硬盘
    * 有一定的数据量，冻结的层数就少一点
    * 大量数据，不进行冻结，只是使用这些参数作为**初始化参数**
* 数据扩充

  * 镜像对称
  * 随机裁剪(Rotation，Shearing，Local warping)
  * 旋转
  * 彩色转换，就是给R,G,B通道上加上不同的失真值(注意PCA对其影响)
  * 可以用CPU多线程进行数据扩充，然后用GPU训练
 {% asset_img resources/0885F9F3C1D49D58CB4CA297DC68E67B %}