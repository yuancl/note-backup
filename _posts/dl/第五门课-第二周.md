---
title: 第五门课-第二周
date: 2018-12-01 18:37:09
tags: 
categories: 
-  深度学习
-  吴恩达课程总结
---

[习题代码:Operations-on-word- vectors](https://github.com/yuancl/dl-algorithm/tree/master/5-RecurrentNeuralNetworks/Week2)

#### 词嵌入
- 背景：one-hot的方式不能表示相近词之间的相似度，比如用语言模型得到 i want a glass of orange juice,如果换成 i want a glass of apple ...，  如果是one-hot就不能得到好的结果
- 原因：one-hot向量通常只有一位为非0，所以它们之间的内积为0，多维空间表示就是垂直的，没有相关性
- 词嵌入：把之前的10000+维度的one-hot转换为300维的相关特征，把该词嵌入300维的空间中
  {% asset_img resources/C41732C6B42FCD3CC1C0DF349DDA0403.jpg %}
  {% asset_img resources/A21F2659F855954FA841C05F9E5F0FB7.jpg %}
- t-SNE算法，将多维空间映射到2为空间中，如上图左图


#### 使用词嵌入(命名实体识别)
- 背景：Robert Lin is an apple farmer 与 Robert Lin is a durian cultivator(Robert Lin 是一个榴莲培育家)榴莲培育家训练样本很少包含该词，很可能不能做到命名实体的识别
- 词嵌入做迁移学习优点及步骤
  - 泛化性比较好
  - 先从大量的文本集中学习词嵌入。一个非常大的文本集，这个可以解决你的训练样本比较少，比如根本没有durian(榴莲)这个词汇的训练样本
  - 你可以用这些词嵌入模型把它迁移到你的新的只有少量标注训练集的任务中，比如说用这个300维的词嵌入来表示你的单词，比one-hot更低维
  - 是否进行微调？如果你有大量的训练数据集可以考虑微调已有的嵌入词
  - 通常迁移任务 A->B，通常是A中有大量数据集，B中只有少量数据集这种情况效果比较好
    - 比如在机器翻译领域词嵌入用得比较少，原因就是我们有大量的翻译样本
- 词嵌入和人脸编码类比
  - 相同点，都是通俗理解都是通过编码，得到标识一个物体的唯一向量
  - 不同点，词嵌入是固定的词汇，而人脸编码是给定任意的图片都可以进行编码
  {% asset_img resources/DA8D7754F10BE0DAA530258811F9AC1E.jpg %}

#### 词嵌入的特性
- 类比推理
  - 解决问题：man对应woman，那么king对应什么？
  - 目标函数：$e_{man}-e_{woman}\approx e_{king}-e_?$,也就是Find work w：argmax sim($e_w, e_{king}-e_{man}+e_{woman}$)
  - 实现算法：
    - 该场景的四个词如果映射为二维空间，一般会是平行四边形，通常只有一维数据有一定差异
    {% asset_img resources/52B643F271BBA6B8880FF8C6499D3894.jpg %}
    - 余弦相识度：
      - 公式：sim(u,v)=$\frac{u^T*v}{||u||_2||v||_2}$=cos($\theta$)
      - 理解：两个向量之间的角度的余弦是衡量它们有多相似的指标，这里就是衡量两个词嵌入向量的相似度
      {% asset_img resources/CAD5599965AB00A4C6BFCD3BBFC387F5.jpg %}
      
#### 嵌入矩阵(Embedding Matrix)
- 当你应用算法来学习词嵌入时，实际上是学习一个嵌入矩阵，我们的目标就是学习一个嵌入矩阵E
- 有了嵌入矩阵，然后乘以one-hot向量，就能够得到$E*o_j=e_j$,$e_j$就是表示我们想要的词嵌入
- 但实践中一般不会$E*o_j=e_j$,$e_j$这样计算，一般会有一个专门的函数来单独查找矩阵E的某列
  {% asset_img resources/C6433150B98AB53156F78848934AB4F2.jpg %}

#### 学习词嵌入
建立一个语言模型是学习词嵌入的好方法
- 方法1：
  $Eo_j=e_j$使用这种方法来学习词嵌入矩阵，训练集中有很多序列语句，然后放入神经网络中，来学习E，这种方法比较复杂，最后会有1800维300*6的向量进入隐藏层，然后进入10000维的softmax层
  {% asset_img resources/34DABF58D294A16191D22C8DED48AEBB.jpg %}
- 方法2(采用固定窗口)：
  只取前4个词(作为一个窗口)来训练，用一个固定的历史窗口就意味着你可以处理任意长度的句子，因为输入的维度(1200)总是固定的
  {% asset_img resources/97D2B0D30C4C77C50ADF9A5BB86C1C70.jpg %}
- 方法3(前后数量不固定)
  可以前后个4个词，
  或者就附件一个词等(Skip-Gram思想)...
  {% asset_img resources/74CF627BF0907DB5F4DD4F0A47FC905E.jpg %}
- 总结：
  如果你真想<font color='red'>建立一个语言模型</font>，用目标词的前几个单词作为上下文是常见做法(上图编号9所示)。但如果你的<font color='red'>目标是学习词嵌入</font>，那么你就可以用这些其他类型的上下文(上图编号10所示)，它们也能得到很好的词嵌入

#### Word2Vec
上面都是讲的通过学习一个神经语言模型来得到更好的词嵌入，而
Word2Vec是更简单高效的方式来学习这种类型的嵌入
- Skip-Gram
  - 在 Skip-Gram模型中，我们要做的是抽取上下文和目标词配对，来构造一个<font color='red'>监督学习问题</font>
    - 但是构造这个监督学习问题的目标并不是想要解决这个监督学习问题本身，而是想要使用这个学习问题来学到一个好的词嵌入模型<font color='red'>(构建一个监督学习问题，实际想解决学习嵌入词模型)</font>
  - 步骤：我们随机选择一个词作为上下文c，然后在随机抽取一个词作为target y<font color='purple'>(随机选择y，我理解只是重点强调这个算法本身，实际情况如果要训练得到一个好的词嵌入，必然是不能随机的，是需要选真实的y)</font>，然后就是建立x(c)-->y的映射
    - c和E相乘，得到c的词嵌入向量，然后此向量通过softmax进行预测，最后和target建立Loss function来达到学习E的目的
  - softmax模型，预测不同目标词的概率:softmax:p(t|c)=$\frac{e^{\theta^{T}_{t}e_c}}{\sum^{10,000}_{j=1}e^{\theta{^T_je_c}}}$输出是10000(词典中单词数量)为的向量
    - $\theta_t$是一个与输出t有关的参数，即某个词t和标签相符的概率是多少
  - softmax损失函数：$L(\hat y,y)=-\sum^{10,000}_{i=1}y_ilog\hat y_i$ <font color='red'>其中y是one-hot向量，$\hat y$是10.000维的各个词的预测输出概率</font>
    - <font color='red'>这是softmax常有的损失模型</font>，$\hat y$是各个分类的概率，可以画一下log函数图形来理解该损失函数，如果$\hat y_i$为1，也就是第i个分类为概率为1，这个时候$L(\hat y,y)$就为0，损失值最小
    - <font color='red'>直观理解损失函数</font>：<font color='blue'>就是y和$\hat y$都是10000维的，Loss计算就是分别将这两个向量做"内积"，要使"内积"最小，假设第i维，$y_i=1$这个时候softmax的输出$\hat y_i=1$->$\log\hat y_i=0$(就是所第i维也正是softmax输出最大的分类的概率)->$y_i\log\hat y_i=0$->Loss最小</font>
  - 缺点(problem)：
    在 softmax 模型中，每次你想要计算这个概率，你需要对你词汇表中的所有 10,000 个词做求和计算
  - 优化方案：
    - 分级softmax分类器：不用一下子告诉属于10,000类中的哪一类，可以先告诉属于左边的5000中的一类还是右边5000中的，然后是2500....,这样来构造一颗分类树
    {% asset_img resources/95F9428AAE4FEA373515DBC5F8A622B8.jpg %}

- CBOW模型
  - 不同版本的 Word2Vec模型，Skip-Gram只是其中的一个，另一个叫做 CBOW，即连续词袋模型(Continuous Bag-Of-Words Model)，它获得中间词两边的的上下 文，然后用周围的词去预测中间的词
  - <font color='red'>CBOW 是从原始语句推测目标字词;而Skip-Gram正好相反，是从目标字词推 测出原始语句</font>
  - CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好
  {% asset_img resources/FF2E1F1A0F94A8221B8B801595C6D5EC.jpg %}

- 负采样(Negative Sampling)
  - 背景：之前的softmax分类器中，解决每次都要计算所有样本的问题
  - Model：选择一个正样本和K个负样本，将softmax的分类转化为一系列二分类问题，只计算其中的正负样本的二分类问题
    - 正负样本作为输入x，然后用$P(y=1|c,t)=sigmoid(\theta^T_te_c)$进行二分类计算
    {% asset_img resources/7D0E816C9C5C70572D286ADC59924886.jpg %}
  - 负样本k的选取
    - 语库中的经验频率进行采样，这种会有很多like，the等高频词出现
    - 另一个极端就是1除以词汇表总词数，均匀的抽取样本，但这样对英语单词的分布是没有代表性的
    - 采用一种处于完全独立分布和训练集的观测分布两个极端之间
    {% asset_img resources/5A531D6FD207171B174D364AF500C77F.jpg %}

#### Glove(Glove word vectors)
- 定义：定义上下文和目标词为任意两个位置相近的单词，假设是左右各 10 词的距离，那么$X_{ij}$就是一个能够获取单词𝑖和单词𝑗出现位置相近时或是彼此接近的频率的计数器。
- Glove模型就是进行优化，将他们之间的差距最小化处理
  - <font color='blue'>就是通过一种优化方法，对目前方程进行优化，从而间接学习到词嵌入矩阵</font>
  {% asset_img resources/E4BFB45184A8FE9F410D8920BDD53ED4.jpg %}
- 函数f选择原则：对加权函数f的选择有着启发性的原则，就是既不给这些词(this，is，of，a)过分的权重，也不给这些不常用词(durion)太小的权值
- 问题：
  - 你不能保证嵌入向量的独立组成部分是能够理解的
  - 你不能保证这些用来表示特征的轴能够等同于人类可能简单理解的轴，具体而言，第一个特征 可能是个Gender、Roya、Age、Food Cost 和 Size的组合<font color='red'>(并不是我们所理解的单纯的Gender维)</font>，它也许是名词或是一个行为动词 和其他所有特征的组合，所以很难看出独立组成部分
  {% asset_img resources/31256B83B3F1A2CF14D942BFECB3ADFB.jpg %}

#### 情感分类
- 定义：情感分类任务就是看一段文本，然后分辨这个人是否喜欢他们在讨论的这个东西
- 简单模型：采用的是平均值单元，适合任意长度的评论
  - <font color='blue'>重点理解词嵌入应用的思想：将单词通过词嵌入矩阵得到词嵌入，简单理解为将字符或者是单词映射为数值的一种方法，然后使用End-to-End思想，通过softmax就直接预测输出</font>
  {% asset_img resources/3769B484BF94E06ABF4038346AA63078.jpg %}
  - 问题：没有考虑词序，<font color='red'>因为均值模型就是把各个单词各个维数值相加然后再平均</font>
    - 比如not good,或者 good...not ,就不能区分了
- RNN模型解决词序问题 
  - 此模型是一个one-many的模型
    - <font color='blue'>直观理解就是，RNN本身就是有一个词一个词有顺序的输入，上一个词对下一个词有影响，所以就能够反映词序问题</font>
  {% asset_img resources/62FD68B5D4C30C44B61209A772F5BD86.jpg %}

#### 词嵌入除偏
- 定义：比如通过词嵌入，Man:Computer Programmer， 同时输出 Woman:Homemaker
- <font color='blue'>解决方法：主要是在词向量中寻找出哪些维度的偏见相关的维度(bias axis)，然后在这些维度上进行调整</font>
- <font color='blue'>重点理解：单词到词向量的映射，实际上就是映射到n维空间(n为词向量矩阵)中的一个点,然后看点与点之间的距离来判断词与词之间的相似度</font>
- <font color='red'>达到目的：让无性别的词到有性别的词的点的距离是相等的,比如Computer Programmer,doctor分别到man,woman的距离是相等的</font>
  - 找出偏见趋势
    - 确定哪些轴(哪些维度)，对于性别歧视这种情况来说，我们能做的是$e_{he} − e_{she}$，因为它们的性别不同，然后 将$e_{male}−e_{female}$，然后将这些值取平均(上图编号2所示)，将这些差简单地求平均。这个趋势(上图编号3所示)看起来就是性别趋势或说是偏见趋势
      - non bias就是垂直于偏见轴的方向
    {% asset_img resources/FE259E9589DE8B2F3259660CB076E522.jpg %}
  - 中和步骤：某些无性别歧视的词语，让他们在non bias轴上进行靠拢，尽量减少映射在bias轴上的距离来减少性别歧视问题
    - 比如Computer Programmer,doctor应该是无性别关系的，所以应该尽量靠近non bias轴，<font color='blue'>这样的理想效果就是到达空间中man和woman的点距离是相等的</font>
    {% asset_img resources/761E18C6CBBE247CE32CCD5CE15F5E54.jpg %}

  - 均衡步：
    让有偏见的词进行，移动与中轴线等距的一些点上，这样让它们在bias轴上都有一致的相似度，图中由1移到图2的两点
      - <font color='blue'>直观理解：也是为了让无偏见词，比如上面说的Computer Programmer,doctor达到分性别关系的词man,woman距离相等</font>
    {% asset_img resources/18EA7A252EAD3B3F90796A808AAAC7F5.jpg %}
    - 经过统计这种偏见词实际上是很少的，需要均衡的词是很少的