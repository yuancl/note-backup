---
title: 第二门课-第三周
date: 2018-09-17 21:10:23
tags: 
categories: 
-  深度学习
-  吴恩达课程总结
---

### 3.1调试处理

* 建议重要性 
  * 学习步长α
  * β(Momentum),hide units,mini-batch size
  * layouts,α的衰减率
  * β1​,β2​,ϵ(Adam)
* 传统ml一般使用grid精确化的选参方法。dl进行随机选择(因为不确定哪些参数重要),采用**由粗精确到细**的方法，范围逐渐变小

### 3.2为超参数选择合适的范围

- 比如说隐藏层层数，可以在坐标轴上2，3，4...选择
- 对数标尺：$[10^a,10^b]$->(a,b)的均值的选择，例如$\beta$(0.9, 0.999)->$1-\beta$(0.1,0.001)即($10^{-1},10^{-3}$)
- 需要根据不同表达式来判断各个超参数的敏感度，比如表达式$\frac{1}{1-\beta}$在$\beta$快接近0的时候，就很敏感

### 3.3超参数训练的实践

* 资源条件允许，进行多参数共同调整训练
* 资源不充足，只能一次训练过程中不断进行参数的调整

### 3.4&3.5&3.6&3.7BN算法思想及应用

* [BatchNorm](https://app.yinxiang.com/shard/s48/nl/12303615/490e3d22-37db-437d-8faf-d97757a35e19/)
* 整体思想就是将每一层的输入都作为新的输入，标准化，使本层能更好更快的迭代
* 加入γ,βscale,shift参数是为了再标准化后一定程度上能保持之前学习到的属性，同时可以去掉每一层的b参数(因为0均值的数，对后面添加的任意值都无意义)
* 它削弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其它层，这有助于加速整个网络的学习

### 3.8&3.9Softmax回归

* softmax与之前的Sigmoid,Relu不同是因为输入为一个n维向量，输出也为n维向量，并且输出是各个C类的概率
* 计算方法：
 {% asset_img resources/806ED3F32E6AABFF0C0B24C0EE04E7C2 %}
* 直观理解：多个线性边界函数进行划分
 {% asset_img resources/33FD90F372D0E3BCD373101FDB1F4A4E %}
* 和hardmax做比较，hardmax是输入一向量，输出为(0,1)的向量组，1为最大概率
* Loss Fun：L(y^​,y)=-∑j=1c​yj​logyj​^​ ---\> L(y^​,y)=-logy^​c​ : 就是你的训练集中的真实类别，然后试图使该类别相应的概率尽可能地高
* 应用到神经网络中:其实就是用你的Softmax激活函数来得到a[l]或者说y

### 3.10深度学习框架

### 3.11Tensorflow

* Tensorflow is a programming framework used in deep learning
* The two main object classes in tensorflow are **Tensors and Operators**.
* When you code in tensorflow you have to take the following steps:

  * Create a graph containing **Tensors** (Variables, Placeholders ...) and **Operations** (tf.matmul, tf.add, ...)
  * Create a session
  * Initialize the session
  * Run the session to execute the graph
* You can execute the graph multiple times as you've seen in model()
* The backpropagation and optimization is automatically done when running the session on the "optimizer" object

**Writing and running programs in TensorFlow has the following steps:**

* Create Tensors (variables) that are not yet executed/evaluated.
* Write operations between those Tensors.
* Initialize your Tensors.
* Create a Session.
* Run the Session. This will run the operations you'd written above.

**computation graph**
