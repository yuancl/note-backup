---
title: ç¬¬äº”é—¨è¯¾-ç¬¬ä¸‰å‘¨
date: 2018-12-06 08:17:19
tags: 
categories: 
-  æ·±åº¦å­¦ä¹ 
-  å´æ©è¾¾è¯¾ç¨‹æ€»ç»“
---

[ä¹ é¢˜ä»£ç :Machine-Translation](https://github.com/yuancl/dl-algorithm/tree/master/5-RecurrentNeuralNetworks/Week3/Machine-Translation)
[ä¹ é¢˜ä»£ç :Trigger-word-detection](https://github.com/yuancl/dl-algorithm/tree/master/5-RecurrentNeuralNetworks/Week3/Trigger-word-detection)

#### åºåˆ—ç»“æ„çš„å„ç§åºåˆ—
- seq2seq
  åˆ†ä¸ºç¼–ç ç½‘ç»œçš„è§£ç ç½‘ç»œ
  {% asset_img resources/4A4F9C5B535DDB65265853ACD2EDC912.jpg %}

- åº”ç”¨åœºæ™¯
  - æœºå™¨ç¿»è¯‘ï¼Œæ¯”å¦‚è‹±æ–‡å’Œæ³•æ–‡çš„äº’ç›¸ç¿»è¯‘
  - Image-to-seq
    ä¾‹å­ä¸­ä½¿ç”¨çš„AlexNetç½‘ç»œï¼Œå°†æœ€åçš„softmaxè¾“å‡ºæ›¿æ¢ä¸ºRNNç½‘ç»œï¼Œè¾“å‡ºä¸ºä¸€ä¸ªåºåˆ—
    {% asset_img resources/49BD4A13FE8A1C4571A631492527A168.jpg %}

- é€‰æ‹©æœ€å¯èƒ½çš„å¥å­
  - è¯­è¨€æ¨¡å‹å’Œç¿»è¯‘æ¨¡å‹æ¯”è¾ƒ
    - è¯­è¨€æ¨¡å‹æ€»æ˜¯ä»¥é›¶å‘é‡å¼€å§‹$(P(y^{< 1 >},y^{< 2 >},y^{< 3 >}...y^{< n >}))$
    - è€Œç¿»è¯‘æ¨¡å‹è¾“å…¥æ˜¯æ³•è¯­çš„encoderï¼Œå¯ä»¥ç†è§£ä¸ºæ¡ä»¶æ¨¡å‹:$P(y^{< 1 >},y^{< 2 >},y^{< 3 >}...y^{< n >}|x^{< 1 >},x^{< 2 >},x^{< 3 >}...x^{< n >})$
    - æ‰€ä»¥ä¸€ä¸ªæ˜¯éšæœºè¾“å‡ºseqï¼Œå¦ä¸€ä¸ªæ˜¯éœ€è¦æ¦‚ç‡æœ€å¤§çš„seq
  {% asset_img resources/BB67CF9637B51A081225158E964A74BA.jpg %}
    - å¦‚ä½•ä¿è¯ç¿»è¯‘æ¨¡å‹ä¸­çš„æ¡ä»¶æ¦‚ç‡æœ€å¤§
      - è´ªå¿ƒç®—æ³•ï¼šæ²¡ä¸€ä¸ªè¾“å‡ºéƒ½ä¿è¯å½“å‰ä¸€ä¸ªterm($y^{<i>}$)æ¦‚ç‡æœ€å¤§ï¼Œä½†æ˜¯å¹¶ä¸èƒ½ä¿è¯<font color='red'>æ•´ä¸ªåºåˆ—æ¦‚ç‡æœ€å¤§</font>ï¼Œæ‰€ä»¥ä¸€æ¬¡æŒ‘é€‰ä¸€ä¸ªè¯å¹¶ä¸æ˜¯æœ€ä½³çš„é€‰æ‹©
      - ç©·ä¸¾æ³•ï¼šå°†æ‰€æœ‰å¥å­è¿›è¡Œç©·ä¸¾æµ‹è¯•ï¼Œä½†ç”±å•è¯ç”Ÿæˆçš„å¥å­é‡å¤ªå¤§ï¼Œæ— æ³•ç©·ä¸¾
      - é›†æŸç®—æ³•

- é›†æŸç®—æ³•(Beam Search)
  - è´ªå©ªç®—æ³•åªä¼šæŒ‘å‡ºæœ€å¯èƒ½çš„é‚£ä¸€ä¸ªå•è¯ï¼Œç„¶åç»§ç»­ã€‚è€Œé›†æŸæœç´¢åˆ™ä¼šè€ƒè™‘å¤šä¸ªé€‰æ‹©ï¼Œé€‰æ‹©æ•°é‡å°±æ˜¯é›†æŸå®½(Beam width)
  - ç¬¬ä¸€æ­¥ï¼š
    éœ€è¦ è¾“å…¥æ³•è¯­å¥å­åˆ°ç¼–ç ç½‘ç»œï¼Œç„¶åä¼šè§£ç è¿™ä¸ªç½‘ç»œï¼Œè¿™ä¸ª softmax å±‚(ä¸Šå›¾ç¼–å· 3 æ‰€ç¤º)ä¼šè¾“ å‡º 10,000 ä¸ªæ¦‚ç‡å€¼ï¼Œå¾—åˆ°è¿™ 10,000 ä¸ªè¾“å‡ºçš„æ¦‚ç‡å€¼ï¼Œå–å‰ä¸‰ä¸ªå­˜èµ·æ¥
  - ç¬¬äºŒæ­¥ï¼š
    - é€‰æ‹©ç¬¬ä¸€æ­¥ä¸­çš„ä¸€ä¸ªè¯$P(y^{<2>}|x, y^{<1>})$å¦‚ä¸‹å›¾3ï¼Œè¾“å…¥ä¸ºç¬¬ä¸€æ­¥çš„è¾“å‡ºå’Œx
    - ä¸ä»…ä»… æ˜¯ç¬¬äºŒä¸ªå•è¯æœ‰æœ€å¤§çš„æ¦‚ç‡ï¼Œè€Œæ˜¯ç¬¬ä¸€ä¸ªã€ç¬¬äºŒä¸ªå•è¯å¯¹æœ‰æœ€å¤§çš„æ¦‚ç‡
    - $P(y^{<1>},y^{<2>}|x)=P(y^{<1>}|x)P(y^{<2>}|x, y^{<1>})$
  - åé¢çš„æ­¥éª¤ä¹Ÿæ˜¯æ¨¡ä»¿ç¬¬äºŒæ­¥è¿›è¡Œè®¡ç®—
  - é‡å¤ç¬¬äºŒæ­¥ï¼Œé€‰æ‹©ç¬¬ä¸€æ­¥ä¸­çš„å…¶ä»–è¯è¿›è¡Œè®¡ç®—
    {% asset_img resources/09E08ACC8EC0182CF63C63DD96E9F0EC.jpg %}
    {% asset_img resources/D81DFB526C95CD2E4362B500F8E26472.jpg %}
  - å¯ä»¥çœ‹è§ï¼Œå¦‚æœé›†æŸå®½ä¸º1ï¼Œé‚£ä¹ˆå…¶å®å°±æ˜¯è´ªå¿ƒç®—æ³•
  
- æ”¹è¿›é›†æŸæœç´¢
  - é—®é¢˜1ï¼š$P(y^{<1>}|x)P(y^{<2>}|x, y^{<1>})$.....ç”±äºæ¯ä¸€æ­¥æ¦‚ç‡å€¼éƒ½å°äº0ï¼Œæ‰€ä»¥æ¯”å®¹æ˜“é€ æˆæ•°å€¼ä¸‹æº¢ï¼Œä¹Ÿå°±æ˜¯å¯¼è‡´ç”µè„‘çš„æµ®ç‚¹è¡¨ç¤ºä¸èƒ½ç²¾ç¡®çš„å­˜å‚¨
  - è§£å†³æ–¹æ¡ˆï¼š
    æˆ‘ä»¬æ€»æ˜¯è®°å½•æ¦‚ç‡çš„å¯¹æ•°å’Œï¼Œè€Œä¸æ˜¯æ¦‚ç‡çš„ä¹˜ç§¯ï¼Œå°±ç”¨åˆ°äº†å¯¹æ•°å‡½æ•°ä¹˜ç§¯çš„æ€§è´¨
  {% asset_img resources/A0E79789CBD92DE575602B479DAD35D3.jpg %}
  
  - é—®é¢˜2ï¼šç”±äºæ¯ä¸€é¡¹ä¹˜ç§¯éƒ½å°äº1ï¼Œå¹¶ä¸”ç›®æ ‡å‡½æ•°æ˜¯æœ€å¤§æ¦‚ç‡çš„è¾“å‡ºï¼Œæ‰€ä»¥ä¸€èˆ¬ä¼šåå‘äºæ¯”è¾ƒçŸ­çš„å¥å­ï¼Œè¿™æ ·ä¹˜ç§¯é¡¹å°±æ¯”è¾ƒå°‘
  - è§£å†³æ–¹æ¡ˆï¼š
    é€šè¿‡é™¤ä»¥ç¿»è¯‘ç»“æœçš„å•è¯æ•°é‡ã€‚è¿™æ ·å°±æ˜¯å–æ¯ä¸ªå•è¯çš„æ¦‚ç‡å¯¹æ•°å€¼çš„å¹³å‡äº†ï¼Œè¿™æ ·å¾ˆæ˜æ˜¾åœ° å‡å°‘äº†å¯¹è¾“å‡ºé•¿çš„ç»“æœçš„æƒ©ç½š
  - æ›´æŸ”å’Œçš„æ–¹æ³•ï¼šå¹¶ä¸ç›´æ¥é™¤ä»¥å•è¯æ•°ï¼Œè€Œæ˜¯ä¼šä¹˜ä¸Šä¸€ä¸ªè¶…å‚æ•°a,æ¯”å¦‚a=0.7ï¼Œå¦‚æœa=1é‚£ä¹ˆå°±ç›¸å½“äºå®Œå…¨ç”¨é•¿åº¦æ¥åšå½’ä¸€åŒ–
  
  - é›†æŸå®½é€‰æ‹©
    é›†æŸå®½è¶Šå¤§æ•ˆæœå½“ç„¶è¶Šå¥½ï¼Œä½†æ˜¯è®¡ç®—ä¹Ÿè¶Šå¤æ‚ã€‚åœ¨äº§å“ä¸­ï¼Œç»å¸¸å¯ä»¥çœ‹åˆ°æŠŠæŸå®½è®¾åˆ° 10
  
#### é›†æŸæœç´¢è¯¯å·®åˆ†æ

#### Bleuå¾—åˆ†
- èƒŒæ™¯ï¼šæ¯”å¦‚æœºå™¨ç¿»è¯‘ä¼šå¾—åˆ°å¾ˆå¤šéƒ½ä¸é”™çš„ç­”æ¡ˆï¼Œé‚£ä¹ˆå¦‚ä½•é€‰æ‹©å‘¢ï¼ŸBleuå¾—åˆ†ç®—æ³•ï¼Œå°±æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå®ƒåšçš„æ˜¯ï¼›ç»™å®šä¸€ä¸ªæœºå™¨ç”Ÿæˆçš„ç¿»è¯‘ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åœ°è®¡ç®—ä¸€ä¸ªç²‰ä¸æ¥è¡¡é‡å¥½å

#### æ³¨æ„åŠ›æ¨¡å‹
- èƒŒæ™¯ï¼šä»Bleuè¯„åˆ†å¯ä»¥çœ‹å‡ºï¼Œé•¿å¥çš„æ•ˆæœæ¯”è¾ƒå·®
  {% asset_img resources/37CDD1E8D8F3D180CB8C734953DD30E0.jpg %}
- åŸç†ï¼šæ¨¡ä»¿äººå·¥ç¿»è¯‘ï¼Œä¸ä¼šä¸€æ¬¡æ€§æŠŠæ•´ä¸ªæ–‡ç« éƒ½è¯»å®Œå†ç¿»è¯‘ï¼Œä¼šè¯»ä¸€éƒ¨åˆ†å†…å®¹ï¼Œç¿»è¯‘ä¸€éƒ¨åˆ†
- ç†è§£ï¼šåœ¨è®¡ç®—å½“å‰æ—¶é—´æ­¥çš„æ—¶å€™ï¼Œä¼šèŠ±å¤šå°‘ç²¾åŠ›å»å…³æ³¨å¤šå°‘å½“å‰è¯é™„è¿‘çš„è¯è¯­
  - è¿™äº›æ˜¯æ³¨æ„åŠ›æƒé‡ï¼Œå³$a^{<t,t'>}$å‘Šè¯‰ä½ ï¼Œå½“ä½ å°è¯•ç”Ÿæˆç¬¬ğ‘¡ä¸ªè‹±æ–‡è¯ï¼Œå®ƒåº”è¯¥èŠ±å¤šå°‘æ³¨æ„åŠ›åœ¨ç¬¬ğ‘¡ä¸ªæ³•è¯­è¯ä¸Šé¢ã€‚å½“ç”Ÿæˆä¸€ä¸ª ç‰¹å®šçš„è‹±æ–‡è¯æ—¶ï¼Œè¿™å…è®¸å®ƒåœ¨æ¯ä¸ªæ—¶é—´æ­¥å»çœ‹å‘¨å›´è¯è·å†…çš„æ³•è¯­è¯è¦èŠ±å¤šå°‘æ³¨æ„åŠ›
  {% asset_img resources/EEBE423431B34A94171A26BDFA7882A4.jpg %}
- ç¼–ç¨‹ç»ƒä¹ ï¼š
  - å°†äººå·¥ç†è§£(Tuesday 09 Oct 1993)çš„æ—¥æœŸç¿»è¯‘ä¸ºæœºå™¨ç†è§£çš„æ—¥æœŸ(1993-10-09)
  - è¿™é‡Œå‰ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä¸ç”¨å–‚ç»™åä¸€ä¸ªè¾“å…¥ï¼Œå¹¶ä¸åƒå‰é¢ç»ƒä¹ çš„æé¾™åå­—ä¸€æ ·ï¼Œå‰åä¸¤ä¸ªå•è¯æ˜¯æœ‰å…³è”çš„
  - åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸€ä¸ªæ—¶é—´æ­¥çš„æ³¨æ„åŠ›é•¿åº¦æ˜¯ä¸ä¸€æ ·çš„
    {% asset_img resources/691EABB718941C596A7B2C25AF3B853D.jpg %}
  - modelï¼š
    æ€»ä½“åˆ†ä¸º2æ­¥ï¼Œ1ï¼šå³è¾¹ï¼Œone "Attention"å¦‚ä½•è®¡ç®—,2:å·¦è¾¹ï¼Œæ•´ä½“çš„attentionæ¨¡å‹
    In this part, you will implement the attention mechanism presented in the lecture videos. Here is a figure to remind you how the model works. The diagram on the left shows the attention model. The diagram on the right shows what one "Attention" step does to calculate the attention variables $\alpha^{\langle t, t' \rangle}$, which are used to compute the context variable $context^{\langle t \rangle}$ for each timestep in the output ($t=1, \ldots, T_y$).
    {% asset_img resources/2868A3ED41699CAA4210EAF9E26E0328.jpg %}
    Here are some properties of the model that you may notice: 

    - æ¨¡å‹åˆ†ä¸¤ä¸ªLSTMå±‚ï¼Œä¸‹é¢ä¸€ä¸ªåŒå‘LSTMè·Ÿè¿›å‰å‘åå‘è®¡ç®—çš„æ¿€æ´»å€¼aï¼Œç„¶ååŒ¹é…ä¸Šå„ä¸ªæ—¶é—´æ­¥çš„æ³¨æ„åŠ›çš„é•¿åº¦ï¼Œäº§å‡ºcontextï¼Œç¬¬äºŒå±‚LSTMæ˜¯ç”¨contextå’Œå‰ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º$S^{<t-1>}$è¾“å‡ºæœ€ç»ˆå€¼
      There are two separate LSTMs in this model (see diagram on the left). Because the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism, we will call it *pre-attention* Bi-LSTM. The LSTM at the top of the diagram comes *after* the attention mechanism, so we will call it the *post-attention* LSTM. The pre-attention Bi-LSTM goes through $T_x$ time steps; the post-attention LSTM goes through $T_y$ time steps. 

    - The post-attention LSTM passes $s^{\langle t \rangle}, c^{\langle t \rangle}$ from one time step to the next. In the lecture videos, we were using only a basic RNN for the post-activation sequence model, so the state captured by the RNN output activations $s^{\langle t\rangle}$. But since we are using an LSTM here, the LSTM has both the output activation $s^{\langle t\rangle}$ and the hidden cell state $c^{\langle t\rangle}$. However, unlike previous text generation examples (such as Dinosaurus in week 1), in this model the post-activation LSTM at time $t$ does will not take the specific generated $y^{\langle t-1 \rangle}$ as input; it only takes $s^{\langle t\rangle}$ and $c^{\langle t\rangle}$ as input. We have designed the model this way, because (unlike language generation where adjacent characters are highly correlated) there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date. 

    - We use $a^{\langle t \rangle} = [\overrightarrow{a}^{\langle t \rangle}; \overleftarrow{a}^{\langle t \rangle}]$ to represent the concatenation of the activations of both the forward-direction and backward-directions of the pre-attention Bi-LSTM. 

    - The diagram on the right uses a `RepeatVector` node to copy $s^{\langle t-1 \rangle}$'s value $T_x$ times, and then `Concatenation` to concatenate $s^{\langle t-1 \rangle}$ and $a^{\langle t \rangle}$ to compute $e^{\langle t, t'}$, which is then passed through a softmax to compute $\alpha^{\langle t, t' \rangle}$. We'll explain how to use `RepeatVector` and `Concatenation` in Keras below. 
  

#### è¯­éŸ³æ¨¡å‹
- CTC cost
  - èƒŒæ™¯ï¼šå¾ˆå¤šæ—¶å€™è¯­éŸ³è¾“å…¥æ¯”è¾“å‡ºä¼šé”™å¾ˆå¤šï¼Œå¦‚ä½•åœ¨RNNä¸­ç›¸åŒè¾“å…¥è¾“å‡ºçš„æ¨¡å‹ä¸­ä½¿ç”¨å‘¢ï¼Ÿ
  - CTC æŸå¤±å‡½æ•°çš„ä¸€ä¸ªåŸºæœ¬è§„åˆ™æ˜¯ å°†ç©ºç™½ç¬¦ä¹‹é—´çš„é‡å¤çš„å­—ç¬¦æŠ˜å èµ·æ¥ï¼Œå†è¯´æ¸…æ¥šä¸€äº›ï¼Œæˆ‘è¿™é‡Œç”¨ä¸‹åˆ’çº¿æ¥è¡¨ç¤ºè¿™ä¸ªç‰¹æ®Šçš„ç©º ç™½ç¬¦(a special blank character)ï¼Œè¿™æ ·å°±å¯ä»¥è®©è¾“å‡ºå’Œè¾“å…¥ç›¸åŒæ•°é‡äº†
  
#### è§¦å‘å­—æ£€æµ‹
- ç”Ÿè°±å›¾ç‰¹å¾å¾—åˆ°ç‰¹å¾å‘é‡$x^{<1>},x^{<2>,...}$
- è®­ç»ƒé›†å¦‚ä½•æ ‡æ³¨ï¼šæ£€æµ‹åˆ°å…³é”®è¯çš„ä»¥åå°±æ ‡æ³¨ä¸º1ï¼Œæœªæ£€æµ‹åˆ°å°±æ ‡æ³¨ä¸º0
  {% asset_img resources/EBCF128EB7F18F6FFFC673EA682EAFE1.jpg %}
- é—®é¢˜ï¼šè¿™æ ·å¯¼è‡´0å’Œ1çš„æ•°é‡å¾ˆä¸å¹³è¡¡ï¼Œç®€å•çš„å¤„ç†æ–¹å¼ï¼šæ¯” èµ·åªåœ¨ä¸€ä¸ªæ—¶é—´æ­¥ä¸Šå»è¾“å‡º 1ï¼Œå…¶å®ä½ å¯ä»¥åœ¨è¾“å‡ºå˜å› 0 ä¹‹å‰ï¼Œå¤šæ¬¡è¾“å‡º 1ï¼Œæˆ–è¯´åœ¨å›ºå®šçš„ ä¸€æ®µæ—¶é—´å†…è¾“å‡ºå¤šä¸ª 1
- ç»ƒä¹ é¢˜ï¼š
  - ç½‘ç»œæ¶æ„
    {% asset_img resources/3927A66C61616AB5B8F15E0F82790B80.jpg %}
  - é¦–å…ˆé€šè¿‡ç”Ÿè°±å›¾å°†å£°éŸ³è½¬æ¢ä¸ºç‰¹å¾å‘é‡
  - ç„¶åå¼€å§‹ä½¿ç”¨äº†CNNï¼Œä½¿5511æ—¶é—´æ­¥å‡å°åˆ°1375ï¼Œè¿™ä¸ªåœ¨RNNä¸­å¤§é‡å‡å°äº†å¤æ‚å¤§
  - ç„¶åç”¨ä¸¤å±‚RNNï¼Œè¿›è¡Œ0ï¼Œ1é¢„æµ‹è¾“å‡ºï¼Œæ³¨æ„å¹¶æ²¡æœ‰ä½¿ç”¨åŒå‘RNNï¼Œå› ä¸ºè¯­éŸ³è¯´å®Œå°±è¦å¿«é€Ÿç»™å‡ºç»“æœï¼Œå¹¶ä¸æ˜¯ç­‰10sè¯´å®Œåæ‰ç»™
  - Here's what you should remember:
    - Data synthesis is an effective way to create a large training set for speech problems, specifically trigger word detection. 
    - Using a spectrogram and optionally a 1D conv layer is a common pre-processing step prior to passing audio data to an RNN, GRU or LSTM.
    - An end-to-end deep learning approach can be used to built a very effective trigger word detection system. 

    