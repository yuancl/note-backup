---
title: 第三门课-第二周
date: 2018-10-10 10:00:09
tags: 
categories: 
-  深度学习
-  吴恩达课程总结
---

#### 进行误差分析：
  * 通过人工check，找出标记错误的样本和总数的百分比，选出比较较大的错误类型进行分析，通常能够帮你找到优化的灵感
  * 深度学习算法对训练集中的随机错误是相对**健壮**的，如果数据够随机，那么不用花太多时间去修正它们
  * 但是如果这些错误比较理解大，**严重影响**到你再开发集上的评估能力，那才真有必要去修正下标注
  * Dev/Test集的分布一定要一致，如果需要更改标注，也是一样。训练集倒可以不一致
  * 最后，深度学习工作者一般很少去人工干预，但是建议也看下误差的影响面

#### 快速搭建你的第一个系统：
  * 如果面对的是新应用，这个时候你的**目标**是得到一个可用的系统，所以可以快速搭建你的系统，然后进行误差分析，找到后面的方向
  * 如果你是老司机，那么可以参考已有的学术论文，已有的架构等构建你的网络

#### 训练集和测试集不同分布：
  * 如果关系App中的图片,即使App的数据很少,也不建议train：webapp，dev/test：app。而建议把一部分收集到的真实数据放入训练集使用train：**webapp+app**, dev/test:app
  * 例如音频数据训练集有500,000条数据，收集到的真实数据是20,000条，那么组合方式有：
  $\begin{cases}
train:500,000+10,000\\
dev:5000\\
test:5000\\
\end{cases}
$ 或者是：
$\begin{cases}
train:500,000\\
dev:10,000\\
test:10,000\\
\end{cases}
$

#### Train/dev数据不同分布带来的偏差和方差问题
  * 问题现象：如果Train/dev数据不同分布，那么如果发现实际dev loss比train大，那么到底是**数据不同分布**导致？还是**方差问题**导致呢？
  * 解决方案：建设一个Train-dev数据集，此数据集合Train数据同分布，来排除是否是**方差的问题**的问题：
  $\begin{cases}
Bias:Human level,Train\\
Variance:Train,Train-dev\\
Datamatch:Train-dev,dev\\
\end{cases}
$
  * 这个时候就有三类问题:**1.bias 2.variance 3.数据不同分布**,对分布不同问题，没有很好的办法，但有一些建议如下: 
    * 人工合成，让你的训练集数据分布接近你的Dev集(比如图像合成) 
      * 问题：可能出现数据集局部过拟合
    * 所以如果是数据分布问题导致，一般分析步骤： 
      * 仔细观察下Train/dev/test数据特点
      * 想办法收集更多想dev/test的数据

#### 迁移学习
  * 预训练(pre-training)：如果你数据比较多，进行全部参数的重新训练，这时你在用图像识别数据去预先初始化，就叫做预训练，或者预训练神经网络权重
  * 微调(fine tuning)：如果以后更新所有权重，然后在其他地方训练，有时这个过程也叫微调
  * 如果没有这么多的数据，也可以选择只替换输出层，或者最后两层等
  * 为什么可以这样迁移？ 
    * 有很多低层次的特征，比如说边缘检测，曲线检测，阳性对象检测等，图像都可以共用的
    * 学到了这些低层次的特征，就能够进一步学习到线条，点，曲面这些知识
  * 适用场景：迁移后的场景，你没有这么多的数据
  * A-\>B条件: 
    * A和B都要有同样的输入，比如都是图像，都是语音等
    * B的数据远小于A
    * 如果觉得A的低层次的特征能够帮助到B

#### 多任务学习(Multi-task learning)
  * 什么叫多任务学习？ 
    * 一个输入会有多个输出flag：比如CV中，自驾驾驶领域，对一个图片的识别既要判断图片是否有交通灯，又要判断行人，车辆等
  * 多任务学习的条件及意义 
    * 类比迁移学习，如果多任务中有共用的低层次的特征，就能够大大提升你的性能，并且比单独每个任务都训练独立的网络效果好很多
    * 类比迁移学习，你多个任务叠加的数据量要比单个任务数据量大很多
    * 要求神经网络足够大，如果过小，那么不见得会比但任务学习有好的性能
  * 应用情况 
    * 实际应用中，迁移学习的使用频率比多任务会大很多，可能多任务在CV领域应用得比较多

#### 端到端学习:
  * 什么是端到端学习？ 
    * 比如说语音识别，从音频输入-\>特征提取-\>...-\>找到音位-\>...-\>构成听写文本，中间有多个阶段。端到端学习就是直接x-\>y的映射，这里就是音频输入-\>构成听写文本
  * 使用条件: 
    * 数据量(x-\>y)直接映射的数据量足够大
  * 优点: 
    * 组件少，中间过程省时省力
  * 缺点: 
    * 很多场景x-\>y直接映射的数据量很少，不好收集
    * 可能会排除了中间很多人为设计的有用的组件，觉得学习算法的指数来源主要有数据和人工设计的东西
  * 例子: 
    * 人脸识别的系统，并没有直接使用x-\>y(门禁照片-\>身份确认)的识别，因为端到端的直接数据并没有这么多数据集。所以拆分为两步：1.识别人脸照片，2.照片和身份的对应。这种方式是因为拆分后的两步都有大量的数据集
    * X射线评估孩子年龄，直接的数据x-\>(x摄像图像-\>孩子年龄)也没有这么多数据，也需要拆分
    * 机器翻译：端到端方法就使用很广泛，因为存在**大量的数据**，比如英文-\>法文....
