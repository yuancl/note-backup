---
title: 第四门课-第一周
date: 2018-10-14 19:30:09
tags: 
categories: 
-  深度学习
-  吴恩达课程总结
---

### 第一周 卷积神经网络

##### 边缘检测

* 浅层特征可能是是垂直检测，水平检测等初级特征，后面层可能是人眼等高级特征，最后是整个人形等
* 理解垂直边缘过滤核：
$
\begin{cases}
1-0--1\\
1-0--1\\
1-0--1\\
\end{cases}
$ 
  * **(1)(3)列正负相互抵消**，只能在差异相差很大的不同区域可能保留比较大的特征值，比如在黑白过渡区域等

##### 更多边缘检测过滤核

* Sobel过滤器 
  * 增加了中间的一行元素的权重，这样使得结果的鲁棒性更会高一些
  -   $\begin{cases}
1-0--1\\
2-0--2\\
1-0--1\\
\end{cases}
$ 
* Scharr过滤器(CV中经常使用) 
  -   $\begin{cases}
3-0--3\\
10-0--10\\
3-0--3\\
\end{cases}
$ 

##### Padding

* 解决问题： 
  * 每次做卷积的时候，他的图像都会缩小，多做几次卷积就输出的图像会很小
  * 图像边缘的像素，只有一个卷积核会计算到，这样一来角落或者图像边缘的信息发挥的作用就会很小
* 通常Padding设置： 
  * Valid(无padding),normal,same(使输出和输入尺寸一致)
  * 一般考虑到对称性和中心点位置，padding值一般使用奇数值

##### 卷积步长(Strided)

##### 三维卷积

* {% asset_img resources/A34E4968C781FBBF7EE500ECE577EE92 %}
* 多filter=通道数=特征数=深度(多种叫法)

##### 单层卷积网络

* 类似神经网络(x-\>z-\>(ReLU+b)-\>a...),可以看出，图像无论多大，卷积核参数都是不变的(这样一定程度能避免过拟合)
* {% asset_img resources/A2DE0F8C6458579436F140074B5FE193 %}

##### 池化层

* 可以理解为卷积核，但是没有参数，比如max pooling,min pooling
* max pooling理解： 
  * 提取最大的数字，可以理解为最突出的特征提取出来，同时如果该局部图像不存在此特征，提取出的值依然很小
  * 4x4就可以看做是特征的集合，最后提取出的9可能就表示然人眼...
 {% asset_img resources/4DDA682D49EE988BEF6230C1A1EFB3D2 %}
  * 超参数(f,s)，一般设置(f=2,s=3)或(f=3,s=2),一般很少会用到Padding，只有超参数并没有其他的参数，没有什么需要学习的，可以理解为只是一个**静态属性**

##### 卷积网络示例

* 一般分类： 
  * 卷积层+池化层 为一层
  * 或者不考虑无参数，无权重的池化层，一层卷积层就为一层
* 常见模式：
 CONV-POOL-CONV-POOL-FC-FC-FC-SOFTMAX
* 特点： 
  * nh​,nw​会逐渐减小，但是信道会增加
  * 激活值也是逐渐减小，但是不能让减小太快，否则会影响网络性能
  * parameters数量不随输入图片的大小而改变，只能卷积核和全连接层有关系(其中parametr数(208=5x5x8+8)(416=5x5x16+16)不和具体输入图像相关)
  * CV研究的工作就是研究如何整合卷积层，池化层，全连接层
  * 网络架构(类似LetNet5)
 {% asset_img resources/EA6A4DE2FF731A127BE54D7B0109DEFA %}
 过程参数变化情况：
 其中parametr数(208=5x5x8+8)(416=5x5x16+16)不和具体输入图像相关
 {% asset_img resources/B5D93F420A7D874EB6AF156E7E1127F6 %}

##### 卷积优点

* 减小参数
* 稀疏连接
